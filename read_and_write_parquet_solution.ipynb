{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2U0y6_MtZ9I",
    "outputId": "8dce3897-0149-490c-fe59-8be822fcaa8a"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KLz5M7sFpO0W"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "# Import the time module so we can time our queries.\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").config(\"spark.driver.memory\", \"2g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CahJMb3cpWdW",
    "outputId": "4d0c57d7-87c3-4463-d0f5-3abfbd2a9e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+-------+-----+-----+----------+-------------------+----------------+------------+--------------------+----------------+--------------------+-------------+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|ISN_DOB_BIS_VIOL|BORO|    BIN|BLOCK|  LOT|ISSUE_DATE|VIOLATION_TYPE_CODE|VIOLATION_NUMBER|HOUSE_NUMBER|              STREET|DISPOSITION_DATE|DISPOSITION_COMMENTS|DEVICE_NUMBER|         DESCRIPTION|ECB_NUMBER|              NUMBER|  VIOLATION_CATEGORY|      VIOLATION_TYPE|\n",
      "+----------------+----+-------+-----+-----+----------+-------------------+----------------+------------+--------------------+----------------+--------------------+-------------+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|         2286033|   1|1009713|00577|00019|  20180507|                  E|     9027/627971|          34|        WEST 14TH ST|        20220509|PPN203 AOC SUB 05...|      1P13420|                null|      null|V*050718E9027/627971|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2533639|   1|1082666|00333|00001|  20210629|                  E|     9027/705433|          77|     COLUMBIA STREET|        20220509|PPN203 AOC SUB 05...|      1P27474|                null|      null|V*062921E9027/705433|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2347979|   1|1083846|01130|00001|  20190423|                  E|     9028/648125|         200|   CENTRAL PARK WEST|        20220509|PPN203 AOC SUBMIT...|      1P40861|                null|      null|V*042319E9028/648125|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2566336|   1|1057155|01889|07502|  20211123|                  E|     9028/710097|         845|        WEST END AVE|        20220509|PPN203 AOC SUB 05...|      1P14972|                null|      null|V*112321E9028/710097|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2487351|   1|1041456|01387|00021|  20200925|                  E|     9028/689200|          31|             E 72 ST|        20220509|PPN203 AOC SUB 05...|      1P10910|                null|      null|V*092520E9028/689200|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2579117|   3|3212907|07717|00054|  20220427|            AEUHAZ1|           00322|        3420|        QUENTIN ROAD|        20220509|000810 PAID INVOI...|         null|FAILURE TO CERTIF...| 39055131L|V*042722AEUHAZ100322|V*-DOB VIOLATION ...|AEUHAZ1-FAIL TO C...|\n",
      "|         2224356|   4|4445407|16155|00021|  20171206|                  E|     9027/617490|         320|    BEACH 100 STREET|        20220509|PPN203 AOC SUBMIT...|       4P4846|                null|      null|V*120617E9027/617490|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         1501541|   1|1010847|00611|00053|  20100706|             LANDMK|         11-0004|          10|      CHARLES STREET|        20220509|                null|         null|                null|      null|V*070610LANDMK11-...|V*-DOB VIOLATION ...|LANDMK-LANDMARK  ...|\n",
      "|         2435215|   1|1012256|00662|00011|  20200115|                  E|     9028/667509|     PIER 59|         NORTH RIVER|        20220509|PPN203 AOC SUB 05...|      1W10006|                null|      null|V*011520E9028/667509|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2504960|   3|3379016|01220|00019|  20210123|                  E|     9028/696197|         715|         ST MARKS AV|        20220509|PPN203 AOC SUB 05...|       3P3165|                null|      null|V*012321E9028/696197|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2579114|   3|3212907|07717|00054|  20220427|            AEUHAZ1|           00319|        3420|        QUENTIN ROAD|        20220509|000810 PAID INVOI...|         null|FAILURE TO CERTIF...| 39055126R|V*042722AEUHAZ100319|V*-DOB VIOLATION ...|AEUHAZ1-FAIL TO C...|\n",
      "|         2347971|   1|1083846|01130|00001|  20190424|                  E|     9028/650040|         200|   CENTRAL PARK WEST|        20220509|PPN203 AOC SUBMIT...|      1P29222|                null|      null|V*042419E9028/650040|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2347976|   1|1083846|01130|00001|  20190423|                  E|     9028/650049|         200|   CENTRAL PARK WEST|        20220509|PPN203 AOC SUBMIT...|      1P40856|                null|      null|V*042319E9028/650049|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2310631|   3|3079907|03439|00020|  20180809|            AEUHAZ1|           00316|         76A|       COOPER STREET|        20220509|            90252351|         null|FAILURE TO CERTIF...| 35331209P|V*080918AEUHAZ100316|V*-DOB VIOLATION ...|AEUHAZ1-FAIL TO C...|\n",
      "|         2582207|   4|4227230|10640|00029|  20220430|                  C|          ER02MA|       91-17|          215 STREET|            null|                null|         null|STRUCTURE RENDERE...|      null|      V043022CER02MA|V-DOB VIOLATION -...|C-CONSTRUCTION   ...|\n",
      "|         2539906|   1|1038758|01337|07502|  20210723|                  E|     9028/704512|           1|UNITED NATIONS PLAZA|        20220509|PPN203 AOC SUBMIT...|      1P37657|                null|      null|V*072321E9028/704512|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         2428141|   3|3425712|06766|07503|  20191102|              BENCH|           01025|        2068|        OCEAN AVENUE|        20220509|  CHALLENGE APPROVED|         null|FAILURE TO FILE B...|      null|  V*110219BENCH01025|V*-DOB VIOLATION ...|BENCH-FAILURE TO ...|\n",
      "|         2543985|   3|3116537|05080|00036|  20210824|                  E|     9028/706976|          21|         ST PAULS CT|        20220509|PPN203 AOC SUB 05...|       3P2286|                null|      null|V*082421E9028/706976|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "|         1705186|   1|1010847|00611|00053|  20120926|             LANDMK|         13-0202|          10|      CHARLES STREET|        20220509|                null|         null|                null|      null|V*092612LANDMK13-...|V*-DOB VIOLATION ...|LANDMK-LANDMARK  ...|\n",
      "|         2533638|   1|1082666|00333|00001|  20210629|                  E|     9027/705432|          77|     COLUMBIA STREET|        20220509|PPN203 AOC SUB 05...|      1P27473|                null|      null|V*062921E9027/705432|V*-DOB VIOLATION ...|E-ELEVATOR       ...|\n",
      "+----------------+----+-------+-----+-----+----------+-------------------+----------------+------------+--------------------+----------------+--------------------+-------------+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Bucket\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/3/NYC_Building_Violations.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"NYC_Building_Violations.csv\"), sep=\",\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S62v26TZLHso",
    "outputId": "5480a615-9014-42fe-ebd9-406add28c876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+----------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "|summary|  ISN_DOB_BIS_VIOL|              BORO|               BIN|             BLOCK|               LOT|         ISSUE_DATE|VIOLATION_TYPE_CODE|VIOLATION_NUMBER|      HOUSE_NUMBER|            STREET|    DISPOSITION_DATE|DISPOSITION_COMMENTS|       DEVICE_NUMBER|         DESCRIPTION|          ECB_NUMBER|            NUMBER|  VIOLATION_CATEGORY|      VIOLATION_TYPE|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+----------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "|  count|           2249357|           2249355|           2246031|           2236515|           2237197|            2249356|            2249357|         2249352|           2248990|           2231958|             1689545|             1648487|             1657117|              709389|              201989|           2249316|             2249316|             2249302|\n",
      "|   mean|1306787.2474680543|2.3464668843595082| 2456521.062597088|2859.5119176387343|466.57405826225846|2.007073744908509E7|               null|             NaN|2783.2523214543485|          53.40625|2.0102321098843772E7| 3.162820579273005E7|            Infinity|            Infinity|       2.104617042E8|              null|                null|                null|\n",
      "| stddev| 743549.2550104005|1.2299498217016895|1297759.7392980421|2880.0568270638696|1742.3216172954549| 103289.90511341594|               null|             NaN| 31076.99272583018|20.639547655395816|   76857.25150439839|1.2270720877515715E8|                 NaN|                 NaN|1.9213306030514506E8|              null|                null|                null|\n",
      "|    min|                 1|                 0|           0000000|             .   6|                 0|           0   0612|                 7A|     #326375C301|            #G25FP|     '\\t15 AVENUE'|            12231998|\"DISMISSED, BC SW...| OBTAIN ALL PERMI...|                   #|                0000|V%*010110UB3135/10|C-CONSTRUCTION   ...|A-SEU            ...|\n",
      "|    25%|          669655.0|               1.0|         1055264.0|             950.0|              10.0|        2.0000207E7|               null|          1464.0|             137.0|              58.0|         2.0051104E7|            266001.0|             60232.0|                 1.0|                 0.0|              null|                null|                null|\n",
      "|    50%|         1309732.0|               2.0|         2806658.0|            1856.0|              30.0|        2.0080305E7|               null|          7534.0|             377.0|              63.0|         2.0120315E7|           1.33388E7|             87139.0|                 1.0|        3.48751536E8|              null|                null|                null|\n",
      "|    75%|         1947759.0|               3.0|         3331517.0|            3767.0|              55.0|        2.0150303E7|               null|         18929.0|            1094.0|              63.0|         2.0160718E7|         6.2037385E7|            815390.0|                 2.0|        3.49874984E8|              null|                null|                null|\n",
      "|    max|            999999|                 `|           9999993|             \\0459|             Q0007|           Y9990120|                  Z|            ¬TVW|               ¦04|       MAIN STREET|            20220619|                   ¦|              X10070|{74}REPLACE MISSI...|                   \\|VWH121602PL106HB01|VWH-VIOLATION WOR...|Z-ZONING         ...|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+----------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the data. \n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7fsnqRDqG2C",
    "outputId": "95a31a73-2ffb-430d-fb22-ec062d106ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|      VIOLATION_TYPE|sum(BORO)|\n",
      "+--------------------+---------+\n",
      "|LL10/80-LOCAL LAW...|   3609.0|\n",
      "|LL11/98-LOCAL LAW...|   9285.0|\n",
      "|HVIOS-NYCHA ELEV ...|    969.0|\n",
      "|P-PLUMBING       ...|  29480.0|\n",
      "|ACH1-(NYCHA) - EL...|   4949.0|\n",
      "|LANDMRK-LANDMARK ...|   5599.0|\n",
      "|LL5-LOCAL LAW 5/7...|   1363.0|\n",
      "|B-BOILER         ...|  17042.0|\n",
      "|FISP-FACADE SAFET...|   6889.0|\n",
      "|EGNCY-EMERGENCY  ...|  12607.0|\n",
      "|ES-ELECTRIC SIGNS...|  18378.0|\n",
      "|                null|    148.0|\n",
      "|L1198-LOCAL LAW 1...|  10656.0|\n",
      "|HBLVIO-HIGH PRESS...|  14628.0|\n",
      "|BENCH-FAILURE TO ...| 110285.0|\n",
      "|RWNRF-RETAINING W...|   4007.0|\n",
      "|FISPNRF-NO REPORT...|  21017.0|\n",
      "|LL2604-PHOTOLUMIN...|    679.0|\n",
      "|LL2604S-SPRINKLER...|   1513.0|\n",
      "|ACJ1-(PRIVATE RES...|   2125.0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 3.2174570560455322 seconds ---\n"
     ]
    }
   ],
   "source": [
    " # Let's create a view with our DataFrame and run SQL that will sum up the boroughs by the type of violation.\n",
    "# We can output the time this step runs in seconds.\n",
    "# Because we are timing the executions, remember to run twice to eliminate the \"load time\" from the discussion.\n",
    "\n",
    "df.createOrReplaceTempView('violations')\n",
    "start_time = time.time()\n",
    "\n",
    "spark.sql(\"\"\"select VIOLATION_TYPE, sum(BORO) from violations group by 1\"\"\").show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g04Evw2jqHoK"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\julia\\OneDrive\\Documents\\2. DATA ANALYTICS - BOOTCAMP\\Module 22-Big-Data\\3\\Activities\\01-Ins_Data_Storage\\Solved\\read_and_write_parquet_solution.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/julia/OneDrive/Documents/2.%20DATA%20ANALYTICS%20-%20BOOTCAMP/Module%2022-Big-Data/3/Activities/01-Ins_Data_Storage/Solved/read_and_write_parquet_solution.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Write out the data in parquet format\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/julia/OneDrive/Documents/2.%20DATA%20ANALYTICS%20-%20BOOTCAMP/Module%2022-Big-Data/3/Activities/01-Ins_Data_Storage/Solved/read_and_write_parquet_solution.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Note: That this is pretty much the same as writing out to a csv to your local directory.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/julia/OneDrive/Documents/2.%20DATA%20ANALYTICS%20-%20BOOTCAMP/Module%2022-Big-Data/3/Activities/01-Ins_Data_Storage/Solved/read_and_write_parquet_solution.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# We are telling Spark to overwrite all of the data if it already exists\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/julia/OneDrive/Documents/2.%20DATA%20ANALYTICS%20-%20BOOTCAMP/Module%2022-Big-Data/3/Activities/01-Ins_Data_Storage/Solved/read_and_write_parquet_solution.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(\u001b[39m'\u001b[39;49m\u001b[39mparquet_violations\u001b[39;49m\u001b[39m'\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mC:\\Users\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[1;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[1;32mC:\\Users\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Users\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Users\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "# Write out the data in parquet format\n",
    "# Note: That this is pretty much the same as writing out to a csv to your local directory.\n",
    "# We are telling Spark to overwrite all of the data if it already exists\n",
    "df.write.parquet('parquet_violations', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYiTD19bHuV4"
   },
   "source": [
    "\n",
    "\n",
    "*   click the folder icon on the left of the notebook to expose the folders and files stored in your colab enviornment.  Notice that a new folder is present with the same name as your parquet file (parquet_title_basic)\n",
    "*   inside of it you will find 'part-*.parquet' files and a '_SUCCESS' file. \n",
    "*  The '_SUCCESS' file is created when Spark creates a Parquet folder\n",
    "*  the part-* files are binary files that store your compressed data in columnar format\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SaDjaQXqnPI"
   },
   "outputs": [],
   "source": [
    "# Read in our new parquet formatted data\n",
    "p_df=spark.read.parquet('parquet_violations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sT7d4hu-q32d"
   },
   "outputs": [],
   "source": [
    "# A parquet formatted DataFrame has all the same methods as a row-based DataFrame\n",
    "# We can convert the DataFrame to a view.\n",
    "p_df.createOrReplaceTempView('p_violations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwpUfAoeq71b",
    "outputId": "d6850e76-4e87-4478-bb41-922a51edc6ce"
   },
   "outputs": [],
   "source": [
    "# Run the same sql as above.  (Note: If you have small datasets it IS possible that times may be very close.)\n",
    "# Because we are timing the executions, remember to run twice to eliminate the \"load time\" from the discussion.\n",
    "\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"select VIOLATION_TYPE, sum(BORO) from p_violations group by 1\"\"\").show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZY1lKBHrjLg"
   },
   "outputs": [],
   "source": [
    "# Writing out a csv file from Spark will also create a folder with \"part\" files.\n",
    "# These files are not binary or compressed and in reality are just normal csv files broken into partitions.\n",
    "# You can see the folder 'out_violations.csv' in your local directory.\n",
    "df.write.csv('out_violations.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a parquet file into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zit0gXHn4Hf2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the parquet_violations folder and get the name of a file and edit the path to the parquet file.  \n",
    "# Check for the correct file name, since the file number will change.\n",
    "parquet_file = \"parquet_violations/part-00000-222c5822-8fe4-4450-97d0-65b5d4a334b4-c000.snappy.parquet\"\n",
    "\n",
    "# Convert the parquet file to a Pandas DataFrame. \n",
    "part_00000_df = pd.read_parquet(parquet_file, engine='auto')\n",
    "part_00000_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Read_and_Write_Parquet_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
